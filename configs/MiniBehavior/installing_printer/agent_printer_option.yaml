---
record:
  # record_rollouts: /hdd/datasets/object_data/breakout/testrun/ball
  record_rollouts: /hdd/datasets/object_data/breakout/run/printer
  record_recycle: -1
  log_filename: logs/temp/printer_train.log
  load_dir: /hdd/datasets/object_data/minibehavior/installing_printer/run
  save_dir: /hdd/datasets/object_data/minibehavior/installing_printer/run
  # checkpoint_dir: /nfs/data/calebc/object_data/breakout/testrun/paddle_ball_checkpoint
  save_interval: 100
environment:
  env: MiniBehavior
  variant: installing_printer
torch:
  gpu: 0
  no_cuda: False
train:
  train: True
  pretrain_frames: 10000 # 
  # pretrain_frames: 10000 # 
  num_steps: 250 #
  num_frames: 1000
  train_edge: agent Action printer
  load_rollouts: /hdd/datasets/object_data/minibehavior/installing_printer/run/agent
  num_iters: 4000
  batch_size: 128 # 
critic_net:
actor_net:
network:
  activation_final: tanh
  scale_logits: 200
  hidden_sizes: 512 512 512
  net_type: mlp
  optimizer:
    lr: .0001
    alt_lr: .0001
    eps: .00001
    alpha: 0.99
    betas: 0.9 0.999
    weight_decay: 0.00
sample:
  sample_type: hist
  param_recycle: 0.0
extract:
  single_obs_setting: 1 1 0 1 0 0
  relative_obs_setting: 1 0 0 0 0
inter:
  interaction_testing: 0.9 -1 -1 -1
option:
  term_form: param
  term_as_done: True
  epsilon_close: 0.6 0.6 3.5 0.01
  param_norm: 1
  constant_lambda: -0.1
  param_lambda: 200
  inter_lambda: 1
  negative_true: True
  temporal_extend: 3
  time_cutoff: 100
  interaction_as_termination: True
  use_binary: True
collect:
  buffer_len : 100000
  test_episode: True
  max_steps: 1000
  prioritized_replay: 0.2 0.4
  aggregator:
    sum_rewards: False
  stream_print_file: logs/mini_behavior/printer/log_dumps/ap_option_stream.txt
hindsight:
  use_her: True
  select_positive: 0.2
  max_hindsight: 20
  interaction_resample: False
  interaction_criteria: 1
  min_replay_len: 3
policy:
  lookahead: 5
  learning_type: ddpg
  tau: .001
  max_critic: 100
  epsilon_random: 0.3
  primacy:
    reset_layers: 4
    reset_frequency: 125
    primacy_iters: 50
    stop_resets: 500
  logging:
    log_interval: 10
    train_log_maxlen: 5
    test_log_maxlen: 50
    initial_trials: 20 #
    test_trials: 10 #
    max_terminate_step: 1 300
  learn:
    grad_epoch: 50
    sample_form: merged
    post_random_iters: 200
action:
  use_relative_action: True
  relative_action_ratio: 0.15
  round_values: True
  # inline:
  #   interaction_config: configs/Breakout/angle_hyperparam/paddle_ball_interaction_inline.yaml
  #   inpolicy_iters: 5000
  #   # inpolicy_schedule: 10
  #   inpolicy_schedule: 150
  #   inpolicy_times: 4
  #   policy_intrain_passive: True
  #   intrain_weighting: 0 200000 10 -1
  #   policy_inline_iters: 20 20 -1
...